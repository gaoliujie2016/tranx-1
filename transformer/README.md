Transformer model implementation from Attention Is All You Need. [1]

## Reference

[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)

[1]
```
@techreport{Vaswani2017,
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
eprint = {1706.03762},
title = {{Attention Is All You Need}},
url = {http://arxiv.org/abs/1706.03762},
year = {2017}
}
```